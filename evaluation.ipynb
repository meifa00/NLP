{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the necessary libraries and modules required for the script. These include libraries for PDF loading, conversational retrieval chains, language models, vector stores, embeddings, and various similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a PDF document from the specified file path using the PyPDFLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vector store using HuggingFace embeddings. It takes chunks of text as input and returns a FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Conversational Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a conversational retrieval chain based on the specified model name. It converts the documents into text chunks, creates a vector store, and then creates a conversational retrieval chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_chain(documents, model_name=\"llama3.1:8b\"):\n",
    "    llm = OllamaLLM(model=model_name)\n",
    "    text_chunks = [doc.page_content for doc in documents]\n",
    "    vectorstore = get_vectorstore(text_chunks)\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever()\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the Jaccard similarity between two vectors. It converts the vectors to sparse matrices, calculates the intersection and union, and returns the Jaccard similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(vec1, vec2):\n",
    "    binarizer = Binarizer()\n",
    "    vec1_bin = binarizer.fit_transform(vec1.toarray()).flatten()\n",
    "    vec2_bin = binarizer.fit_transform(vec2.toarray()).flatten()\n",
    "    return jaccard_score(vec1_bin, vec2_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models Based on Similarity and Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the models based on various similarity metrics and the time taken. It calculates cosine similarity using TF-IDF and CountVectorizer, Jaccard similarity, and the time taken for each model. It then determines the best model based on similarity scores and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_time_and_similarity(models_responses, true_answers):\n",
    "    results = {\n",
    "        \"Cosine Similarity (TF-IDF)\": {},\n",
    "        \"Cosine Similarity (CountVectorizer)\": {},\n",
    "        \"Jaccard Similarity\": {},\n",
    "        \"Time Taken (seconds)\": {}\n",
    "    }\n",
    "    averages = {}\n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "    best_time = float(\"inf\")\n",
    "\n",
    "    for model_name, responses in models_responses.items():\n",
    "        start_time = time.time()\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        count_vectorizer = CountVectorizer()\n",
    "\n",
    "        tfidf_all = tfidf_vectorizer.fit_transform(responses + true_answers)\n",
    "        tfidf_responses = tfidf_all[:len(responses)]\n",
    "        tfidf_answers = tfidf_all[len(responses):]\n",
    "\n",
    "        count_all = count_vectorizer.fit_transform(responses + true_answers)\n",
    "        count_responses = count_all[:len(responses)]\n",
    "        count_answers = count_all[len(responses):]\n",
    "\n",
    "        tfidf_cosine_scores = [\n",
    "            cosine_similarity(tfidf_responses[i], tfidf_answers[i])[0][0]\n",
    "            for i in range(len(responses))\n",
    "        ]\n",
    "\n",
    "        count_cosine_scores = [\n",
    "            cosine_similarity(count_responses[i], count_answers[i])[0][0]\n",
    "            for i in range(len(responses))\n",
    "        ]\n",
    "\n",
    "        jaccard_scores = [\n",
    "            calculate_jaccard_similarity(count_responses[i], count_answers[i])\n",
    "            for i in range(len(responses))\n",
    "        ]\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        results[\"Cosine Similarity (TF-IDF)\"][model_name] = tfidf_cosine_scores\n",
    "        results[\"Cosine Similarity (CountVectorizer)\"][model_name] = count_cosine_scores\n",
    "        results[\"Jaccard Similarity\"][model_name] = jaccard_scores\n",
    "        results[\"Time Taken (seconds)\"][model_name] = time_taken\n",
    "\n",
    "        avg_tfidf_cosine = sum(tfidf_cosine_scores) / len(tfidf_cosine_scores)\n",
    "        avg_count_cosine = sum(count_cosine_scores) / len(count_cosine_scores)\n",
    "        avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "\n",
    "        average_score = (avg_tfidf_cosine + avg_count_cosine + avg_jaccard) / 3\n",
    "        averages[model_name] = {\n",
    "            \"Average Similarity Score\": average_score,\n",
    "            \"Time Taken\": time_taken\n",
    "        }\n",
    "\n",
    "        if average_score > best_score or (average_score == best_score and time_taken < best_time):\n",
    "            best_model = model_name\n",
    "            best_score = average_score\n",
    "            best_time = time_taken\n",
    "\n",
    "    for metric, scores in results.items():\n",
    "        print(f\"\\n{metric}:\\n{scores}\")\n",
    "\n",
    "    print(f\"\\nBest Model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function loads the PDF document, defines the questions and true answers, creates conversational chains for different models, collects responses, and evaluates the models based on similarity metrics and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_32108\\2118693373.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "d:\\Interactive Chat Application for Conversations with PDF or Document Content Using a Language Model (LLM)\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_32108\\554169524.py:24: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run({\"question\": question, \"context\": documents, \"chat_history\": []})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity (TF-IDF):\n",
      "{'llama3.1:8b': [0.3465677775290853, 0.1419444731784938, 0.17618832884564495], 'gemma2:9b': [0.3073037827292642, 0.18903665694069782, 0.20695495106776807], 'mistral:7b': [0.2613714813028336, 0.10176313430406181, 0.13964412755566039], 'qwen2:7b': [0.25832594910678336, 0.23651085808129835, 0.17179564910579878]}\n",
      "\n",
      "Cosine Similarity (CountVectorizer):\n",
      "{'llama3.1:8b': [0.580033831501078, 0.2748282859651492, 0.3581143573661566], 'gemma2:9b': [0.551931511524247, 0.27459450972682964, 0.4202555931245649], 'mistral:7b': [0.4893617568266501, 0.19127301391900148, 0.23148258403413785], 'qwen2:7b': [0.4252175442421335, 0.3095517423099226, 0.3217598666159375]}\n",
      "\n",
      "Jaccard Similarity:\n",
      "{'llama3.1:8b': [0.09285714285714286, 0.06097560975609756, 0.039735099337748346], 'gemma2:9b': [0.09016393442622951, 0.04819277108433735, 0.05217391304347826], 'mistral:7b': [0.09090909090909091, 0.05970149253731343, 0.05952380952380952], 'qwen2:7b': [0.05907172995780591, 0.061855670103092786, 0.036458333333333336]}\n",
      "\n",
      "Time Taken (seconds):\n",
      "{'llama3.1:8b': 0.29564976692199707, 'gemma2:9b': 0.011031866073608398, 'mistral:7b': 0.008999824523925781, 'qwen2:7b': 0.010292530059814453}\n",
      "\n",
      "Best Model: gemma2:9b\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = 'pdf/Root_Base_Law_of_Root_Sep_5_2024.pdf'\n",
    "    documents = load_pdf(file_path)\n",
    "\n",
    "    questions = [\n",
    "        \"I’d like to do something, and the rules don’t say that I can’t do it. Can I do it?\",\n",
    "        \"Can another player not consent to an action?\",\n",
    "        \"It seems like something should happen, but the rule doesn’t tell me to do that thing. What do I do?\"\n",
    "    ]\n",
    "\n",
    "    true_answers = [\n",
    "        \"Within the confines of the action, yes! The game will often surprise you with outlandish, unexpected situations, and that’s part of the fun, but this doesn’t mean you can flip the table.\",\n",
    "        \"No actions require consent. Just do the thing.\",\n",
    "        \"Follow the literal word of the Law, not your instinct, even if a similar rule exists.\"\n",
    "    ]\n",
    "\n",
    "    models_responses = {}\n",
    "    models = [\"llama3.1:8b\", \"gemma2:9b\", \"mistral:7b\", \"qwen2:7b\"]\n",
    "\n",
    "    for model in models:\n",
    "        qa_chain = create_conversational_chain(documents, model_name=model)\n",
    "        model_responses = []\n",
    "        for question in questions:\n",
    "            response = qa_chain.run({\"question\": question, \"context\": documents, \"chat_history\": []})\n",
    "            model_responses.append(response)\n",
    "        models_responses[model] = model_responses\n",
    "\n",
    "    evaluate_models_time_and_similarity(models_responses, true_answers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
